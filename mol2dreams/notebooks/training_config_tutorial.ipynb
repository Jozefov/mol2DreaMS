{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this section, we’ll explore the training part of the YAML configuration file. This configuration defines how the model training process is set up, including specifying the trainer type, loss functions, datasets, optimizers, and other training parameters.\n",
    "\n",
    "Here’s the training section from your YAML file:\n",
    "```yaml\n",
    "training:\n",
    "  trainer:\n",
    "    type: 'AdversarialTrainer'\n",
    "    params:\n",
    "      lambda_triplet: 1.0\n",
    "      lambda_adv: 1.0\n",
    "      lambda_gp: 10.0\n",
    "      use_gradient_penalty: true\n",
    "      discriminator:\n",
    "        embedding_dim: 1024\n",
    "  num_epochs: 10\n",
    "  validate_every: 2\n",
    "  save_every: 2\n",
    "  save_best_only: true\n",
    "  device: 'cpu'\n",
    "  log_dir: 'path_to_log_directory'\n",
    "  loss_function:\n",
    "    type: 'TripletCosineLoss'\n",
    "    params:\n",
    "      margin: 0.1\n",
    "      normalize: false\n",
    "  optimizer:\n",
    "    type: 'Adam'\n",
    "    params:\n",
    "      lr: 0.001\n",
    "  discriminator_optimizer:\n",
    "    type: 'Adam'\n",
    "    params:\n",
    "      lr: 0.001\n",
    "  train_loader:\n",
    "    path: 'path_to_triplet_train_dataset.pt'\n",
    "    dataset_type: 'TripletDataset'\n",
    "    batch_size: 32\n",
    "    num_workers: 0\n",
    "    shuffle: true\n",
    "  val_loader:\n",
    "    path: 'path_to_triplet_val_dataset.pt'\n",
    "    dataset_type: 'TripletDataset'\n",
    "    batch_size: 32\n",
    "    num_workers: 0\n",
    "    shuffle: false\n",
    "  test_loader:\n",
    "    path: 'path_to_triplet_test_dataset.pt'\n",
    "    dataset_type: 'TripletDataset'\n",
    "    batch_size: 32\n",
    "    num_workers: 0\n",
    "    shuffle: false\n",
    "```\n",
    "#### Overview\n",
    "\n",
    "\t•\tTrainer Specification: Choose the trainer class to use and set its parameters.\n",
    "\t•\tTraining Parameters: Define epochs, validation frequency, saving frequency, etc.\n",
    "\t•\tLoss Function: Specify the loss function and its parameters.\n",
    "\t•\tOptimizers: Define optimizers for the model and discriminator (if applicable).\n",
    "\t•\tData Loaders: Set up the training, validation, and test datasets.\n",
    "\n",
    "#### 1. Specifying the Trainer\n",
    "\n",
    "Available Trainers\n",
    "\n",
    "You have three trainer classes available:\n",
    "\n",
    "\t1.\tTrainer: Basic trainer suitable for simple tasks.\n",
    "\t2.\tTripletTrainer: Designed for triplet-based training using triplet losses.\n",
    "\t3.\tAdversarialTrainer: Combines triplet loss with adversarial training.\n",
    "\n",
    "Choosing the Trainer\n",
    "\n",
    "In your YAML configuration:\n",
    "\n",
    "```\n",
    "trainer:\n",
    "  type: 'AdversarialTrainer'\n",
    "  params:\n",
    "    lambda_triplet: 1.0\n",
    "    lambda_adv: 1.0\n",
    "    lambda_gp: 10.0\n",
    "    use_gradient_penalty: true\n",
    "    discriminator:\n",
    "      embedding_dim: 1024\n",
    "```\n",
    "\t•\ttype: Specifies the trainer class to use.\n",
    "\t•\tparams: Additional parameters specific to the chosen trainer.\n",
    "\n",
    "Trainer-Specific Parameters\n",
    "\n",
    "AdversarialTrainer\n",
    "\n",
    "\t•\tlambda_triplet: Weight for the triplet loss component.\n",
    "\t•\tlambda_adv: Weight for the adversarial loss component.\n",
    "\t•\tlambda_gp: Weight for the gradient penalty term.\n",
    "\t•\tuse_gradient_penalty: Boolean flag to enable or disable gradient penalty.\n",
    "\t•\tdiscriminator: Configuration for the discriminator network.\n",
    "\t•\tembedding_dim: Dimension of the embeddings used in the discriminator.\n",
    "\n",
    "Other Trainers\n",
    "\n",
    "\t•\tTrainer: May not require additional parameters.\n",
    "\t•\tTripletTrainer: Might have its own specific parameters.\n",
    "\n",
    "#### 2. Mapping Trainers to Datasets and Loss Functions\n",
    "\n",
    "Datasets\n",
    "\n",
    "\t•\tSimpleDataset:\n",
    "\t•\tContains a list of graph data objects.\n",
    "\t•\tUsed with the basic Trainer.\n",
    "\t•\tCompatible with losses that require only inputs and targets (e.g., MSELoss, CosineEmbeddingLoss).\n",
    "\t•\tTripletDataset:\n",
    "\t•\tContains triplets of data: anchor, positive, and negative examples.\n",
    "\t•\tUsed with TripletTrainer and AdversarialTrainer.\n",
    "\t•\tCompatible with triplet-based losses (e.g., TripletMarginLoss, TripletCosineLoss).\n",
    "\n",
    "Loss Functions\n",
    "\n",
    "\t•\tLosses for SimpleDataset and Trainer:\n",
    "\t•\tMSELoss\n",
    "\t•\tCosineEmbeddingLoss\n",
    "\t•\tLosses for TripletDataset and TripletTrainer/AdversarialTrainer:\n",
    "\t•\tTripletMarginLoss\n",
    "\t•\tTripletCosineLoss\n",
    "\n",
    "Combining Trainers, Datasets, and Losses\n",
    "\n",
    "\t•\tBasic Trainer with SimpleDataset:\n",
    "\t•\tSuitable for regression tasks or embedding matching.\n",
    "\t•\tUse with MSELoss or CosineEmbeddingLoss.\n",
    "\t•\tTripletTrainer with TripletDataset:\n",
    "\t•\tFor training models using triplet losses to enforce relative similarity.\n",
    "\t•\tUse with TripletMarginLoss or TripletCosineLoss.\n",
    "\t•\tAdversarialTrainer with TripletDataset:\n",
    "\t•\tCombines triplet loss with adversarial training.\n",
    "\t•\tSuitable for tasks requiring both embedding alignment and adversarial robustness.\n",
    "\n",
    "#### 3. Loss Function Configuration\n",
    "\n",
    "In your YAML:\n",
    "```\n",
    "loss_function:\n",
    "  type: 'TripletCosineLoss'\n",
    "  params:\n",
    "    margin: 0.1\n",
    "    normalize: false\n",
    "```\n",
    "\t•\ttype: The loss function class to use.\n",
    "\t•\tparams: Parameters specific to the loss function.\n",
    "\n",
    "Example: TripletCosineLoss\n",
    "\n",
    "\t•\tmargin: The margin enforced between positive and negative pairs.\n",
    "\t•\tnormalize: Whether to normalize embeddings before computing cosine similarity.\n",
    "\n",
    "#### 4. Optimizer Configuration\n",
    "\n",
    "Model Optimizer\n",
    "```\n",
    "optimizer:\n",
    "  type: 'Adam'\n",
    "  params:\n",
    "    lr: 0.001\n",
    "```\n",
    "\t•\ttype: The optimizer class to use for the model.\n",
    "\t•\tparams: Parameters specific to the optimizer (e.g., learning rate).\n",
    "\n",
    "Discriminator Optimizer (AdversarialTrainer)\n",
    "```\n",
    "discriminator_optimizer:\n",
    "  type: 'Adam'\n",
    "  params:\n",
    "    lr: 0.001\n",
    "```\n",
    "\t•\tOnly required when using AdversarialTrainer.\n",
    "\t•\tUsed to optimize the discriminator network separately.\n",
    "\n",
    "#### 5. Data Loader Configuration\n",
    "\n",
    "Dataset Preparation\n",
    "\n",
    "\t•\tSimpleDataset:\n",
    "\t•\tPrepared in massspec_dreams_embedding.ipynb.\n",
    "\t•\tContains data like:\n",
    "```\n",
    "Data(x=[64, 84], edge_index=[2, 128], edge_attr=[128, 7], y=[1, 1024],\n",
    "     IDENTIFIER=[1], COLLISION_ENERGY=[1, 1], adduct=[1], precursor_mz=[1, 1])\n",
    "```\n",
    "\n",
    "\t•\tTripletDataset:\n",
    "\t•\tPrepared in TripletMarginLoss_dataset_construction.ipynb.\n",
    "\t•\tContains triplets of the above data structures.\n",
    "\n",
    "Data Loader Configuration in YAML\n",
    "\n",
    "Training Data Loader\n",
    "```\n",
    "train_loader:\n",
    "  path: 'path_to_triplet_dataset.pt'\n",
    "  dataset_type: 'TripletDataset'\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  shuffle: true\n",
    "```\n",
    "\t•\tpath: Path to the saved dataset file.\n",
    "\t•\tdataset_type: Type of the dataset class (SimpleDataset or TripletDataset).\n",
    "\t•\tbatch_size: Number of samples per batch.\n",
    "\t•\tnum_workers: Number of subprocesses to use for data loading.\n",
    "\t•\tshuffle: Whether to shuffle the data at every epoch.\n",
    "\n",
    "Validation and Test Data Loaders\n",
    "\n",
    "\t•\tSimilar to the training data loader but typically with shuffle: false.\n",
    "\n",
    "#### 6. Training Parameters\n",
    "```\n",
    "num_epochs: 10\n",
    "validate_every: 2\n",
    "save_every: 2\n",
    "save_best_only: true\n",
    "device: 'cpu'\n",
    "log_dir: 'path_to_log_directory'\n",
    "```\n",
    "\t•\tnum_epochs: Total number of training epochs.\n",
    "\t•\tvalidate_every: Perform validation every N epochs.\n",
    "\t•\tsave_every: Save model checkpoints every N epochs.\n",
    "\t•\tsave_best_only: If true, only save the model when it achieves a new best validation loss.\n",
    "\t•\tdevice: Device to use for training ('cpu' or 'cuda').\n",
    "\t•\tlog_dir: Directory to save logs and model checkpoints.\n",
    "\n",
    "#### 7. Understanding the Data Structures\n",
    "\n",
    "SimpleDataset Data Structure\n",
    "\n",
    "\t•\tEach element is a Data object containing:\n",
    "\t•\tx: Node feature matrix.\n",
    "\t•\tedge_index: Graph connectivity in COO format.\n",
    "\t•\tedge_attr: Edge feature matrix.\n",
    "\t•\ty: Target embedding (e.g., DreaMS spectral embedding).\n",
    "\t•\tAdditional Attributes: Such as IDENTIFIER, COLLISION_ENERGY, adduct, precursor_mz.\n",
    "\n",
    "TripletDataset Data Structure\n",
    "\n",
    "\t•\tEach element is a tuple of three Data objects: (anchor, positive, negative).\n",
    "\t•\tUsed for training with triplet losses.\n",
    "\n",
    "#### 8. Dataset Preparation\n",
    "\n",
    "\t•\tSimpleDataset:\n",
    "\t•\tPrepared using the massspec_dreams_embedding.ipynb notebook.\n",
    "\t•\tInvolves featurizing molecules and associating them with embeddings.\n",
    "\t•\tTripletDataset:\n",
    "\t•\tPrepared using the TripletMarginLoss_dataset_construction.ipynb notebook.\n",
    "\t•\tBased on approaches used in DreaMS for contrastive fine-tuning for atlas prediction.\n",
    "\t•\tTriplets are formed to ensure meaningful anchor-positive-negative relationships.\n",
    "\n",
    "#### 9. Setting Up the YAML for Training\n",
    "\n",
    "When setting up the training section in your YAML configuration, follow these steps:\n",
    "\n",
    "\t1.\tChoose the Trainer:\n",
    "\t\tDecide on the trainer type based on your task and dataset.\n",
    "\t\tIf using triplet losses and triplet datasets, choose TripletTrainer or AdversarialTrainer.\n",
    "\t\tFor simple regression or matching tasks, use the basic Trainer.\n",
    "\t2.\tConfigure the Loss Function:\n",
    "\t\tSelect the appropriate loss function for your trainer and dataset.\n",
    "\t\tSet any necessary parameters specific to the loss.\n",
    "\t3.\tSet Up the Optimizers:\n",
    "\t\tDefine the optimizer for the model.\n",
    "\t\tIf using AdversarialTrainer, also define the optimizer for the discriminator.\n",
    "\t4.\tPrepare and Specify the Datasets:\n",
    "\t\tEnsure your datasets are prepared and saved at the specified paths.\n",
    "\t\tUse the correct dataset_type in the data loader configurations.\n",
    "\t5.\tDefine Training Parameters:\n",
    "\t\tSet the number of epochs, validation frequency, saving frequency, etc.\n",
    "\t\tChoose the device for training ('cpu' or 'cuda').\n",
    "\t6.\tAdditional Trainer Parameters:\n",
    "\t\tIf using AdversarialTrainer, specify parameters like lambda_triplet, lambda_adv, etc.\n",
    "\t\tThese control the weighting of different loss components and training behaviors.\n",
    "\n",
    "#### 10. Example Configurations\n",
    "\n",
    "Using Basic Trainer with SimpleDataset\n",
    "```\n",
    "trainer:\n",
    "  type: 'Trainer'\n",
    "  params: {}\n",
    "loss_function:\n",
    "  type: 'MSELoss'\n",
    "  params: {}\n",
    "train_loader:\n",
    "  path: 'path_to_simple_dataset.pt'\n",
    "  dataset_type: 'SimpleDataset'\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  shuffle: true\n",
    "```\n",
    "\t•\tSuitable for regression tasks where you want to minimize the mean squared error between model outputs and targets.\n",
    "\n",
    "Using TripletTrainer with TripletDataset\n",
    "```\n",
    "trainer:\n",
    "  type: 'TripletTrainer'\n",
    "  params: {}\n",
    "loss_function:\n",
    "  type: 'TripletMarginLoss'\n",
    "  params:\n",
    "    margin: 1.0\n",
    "train_loader:\n",
    "  path: 'path_to_triplet_train_dataset.pt'\n",
    "  dataset_type: 'TripletDataset'\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  shuffle: true\n",
    "```\n",
    "\t•\tSuitable for training models using triplet loss to enforce relative similarity.\n",
    "\n",
    "Using AdversarialTrainer with TripletDataset\n",
    "```\n",
    "trainer:\n",
    "  type: 'AdversarialTrainer'\n",
    "  params:\n",
    "    lambda_triplet: 1.0\n",
    "    lambda_adv: 1.0\n",
    "    lambda_gp: 10.0\n",
    "    use_gradient_penalty: true\n",
    "    discriminator:\n",
    "      embedding_dim: 1024\n",
    "loss_function:\n",
    "  type: 'TripletCosineLoss'\n",
    "  params:\n",
    "    margin: 0.1\n",
    "    normalize: false\n",
    "train_loader:\n",
    "  path: 'path_to_triplet_train_dataset.pt'\n",
    "  dataset_type: 'TripletDataset'\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  shuffle: true\n",
    "```\n",
    "\t•\tCombines triplet loss with adversarial training for more robust embedding alignment.\n",
    "\n",
    "#### 11. Additional Notes\n",
    "\n",
    "\t•\tGradient Penalty in AdversarialTrainer:\n",
    "\t•\tThe gradient penalty term helps in stabilizing the adversarial training by penalizing the model when the gradients have high norms.\n",
    "\t•\tControlled by lambda_gp and can be enabled or disabled using attribute use_gradient_penalty.\n",
    "\t•\tDiscriminator Configuration:\n",
    "\t•\tWhen using AdversarialTrainer, you need to define the discriminator network’s parameters.\n",
    "\t•\tThe embedding_dim should match the dimension of the embeddings produced by your model.\n",
    "\t•\tLogging and Checkpointing:\n",
    "\t•\tThe log_dir specifies where logs and model checkpoints are saved.\n",
    "\t•\tModel checkpoints are saved at intervals defined by save_every and can be configured to save only the best model using save_best_only.\n",
    "\t•\tDevice Selection:\n",
    "\t•\tSpecify 'cpu' or 'cuda' depending on whether you want to train on CPU or GPU.\n",
    "\n",
    "#### Proceeding with Training\n",
    "\n",
    "Once your YAML configuration is set up:\n",
    "\n",
    "\t1.\tLoad the Configuration:\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "\n",
    "\t2.\tBuild the Model and Trainer:\n",
    "\n",
    "from mol2dreams.utils.parser import build_trainer_from_config\n",
    "\n",
    "trainer = build_trainer_from_config(config)\n",
    "\n",
    "\n",
    "\t3.\tStart Training:\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\t4.\tEvaluate the Model:\n",
    "\n",
    "trainer.test()\n",
    "\n"
   ],
   "id": "262989d5132018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f99b374ad7617c8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
