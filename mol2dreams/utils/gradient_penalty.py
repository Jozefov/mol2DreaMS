import torch

def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):
    """
    Calculates the gradient penalty loss for WGAN GP.

    Args:
        discriminator: The discriminator model.
        real_samples: Real data samples.
        fake_samples: Fake data samples generated by the generator.
        device: Device to perform computations on.

    Returns:
        Gradient penalty loss.
    """
    # Random weight term for interpolation between real and fake samples
    alpha = torch.rand(real_samples.size(0), 1).to(device)
    alpha = alpha.expand_as(real_samples)

    # Interpolated samples
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)

    # Discriminator output on interpolated samples
    d_interpolates = discriminator(interpolates)

    # Compute gradients w.r.t. interpolated samples
    gradients = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients = gradients.view(gradients.size(0), -1)
    gradient_norm = gradients.norm(2, dim=1)
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()
    return gradient_penalty