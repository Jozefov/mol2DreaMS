- Use trelib library to represent fragments
- Each fragment has modes ("[M+H]+", "[M]+", "[M-H]+", "[M-2H]+", "[M-3H]+"])

- Each fragment has weight plus adduct, and also for positive and negative ionisation

- build_fragmentation_tree is recursive structure
- build_fragmentation_tree use create_Fragments on each step

- create_Fragments use break_bond to break 

- build_fragmentation_tree_by_single_edge_breaks use create_fragments, instead of create_Fragments

- bond break use rdkit.Chem.rdchem.EditableMol editable molecule class, and remove bond I,

- GetMolFrags: Finds the disconnected fragments from a molecule.

- Fragment, edge is tuple (i,j)

- edges to build_fragmentation_tree are from adj_matrix get_edges, (j, edge_to[I])

- use set isotopes (setIsotope()) for atom identification, as variable. It disguise true value of isotope, it just assigns unique variable, number to atoms


QUESTIONS:
 Isotops, dummy_atoms, ring fragmentation not solved (len(fragments) < 1)
What is this add_dummy_atoms:  em.AddAtom(Chem.Atom(0))
====================================================

Bond investigation:

break_tendency.ipynb


==================================================

 METABOLITE.PY:

- in as_geometric_data convert metabolite object to graph representation, use edge_break_labels, from where?

- match_peak_list
- match_fragments_to_peaks (here some statistics about fragmentation)
- serach peak_matches
- search edge_break
	probably info_graph
- bond encoder


================================================

	SimulationFramework
- predict_metabolite_property convert metabolite object to graph representation




Then search: pred = fiora.simulate_and_score(metabolite, model=model) how does it learn to fragment

=================================================

CHATGPT response: 
The functions you've highlighted from `Metabolite.py` and `FragmentationTree.py` are indeed used to gather and calculate statistics about molecular fragmentation, which are then used for training and prediction in the fragmentation model. Let’s go through how this information is used and linked to the training process:

### Overview of Key Functions and Their Purpose:

1. **`match_fragments_to_peaks` in `Metabolite.py`**:
   - This function matches observed mass-to-charge ratio (`m/z`) peaks from experimental data with possible fragments generated by the fragmentation tree. It calculates various statistics and probabilities associated with the likelihood of each bond (edge) breaking within the molecule.
   - Key statistics gathered include:
     - **Edge Break Counts**: Counts how often each bond (edge) in the molecule is associated with a detected fragment.
     - **Edge Intensity Distributions**: Distributes observed peak intensities across edges based on fragment associations, which helps in understanding which bonds are most likely to break.
     - **Probability Vectors**: Compiles different probability distributions (`compiled_probs`, `compiled_probs2`, etc.) that describe the likelihood of each bond breaking under various conditions or ionization modes.

2. **`match_peak_list` in `FragmentationTree.py`**:
   - This function matches each experimental peak with potential fragments from the fragmentation tree. It calculates relative intensities and other statistics that can be used to refine fragmentation predictions.
   - It plays a critical role in creating a mapping between experimental data and simulated fragmentations, which is necessary for model training.

### How These Functions Feed into Model Training:

1. **Training Data Preparation**:
   - The statistics and probabilities calculated by `match_fragments_to_peaks` are used as training labels. For instance:
     - **`self.compiled_probsALL`**: This attribute, which represents the end-to-end predicted probabilities for bond breaks, is used as a key label for training the model.
     - Other compiled probability vectors (`compiled_probs`, `compiled_probs2`, `compiled_probs6`, etc.) may also be used as training targets, depending on the specific model setup and training requirements.

2. **Feeding Data to the Model**:
   - In the `SimulationFramework.py`, particularly within the `predict_metabolite_property` method, the `Metabolite` object is converted into a graph representation suitable for a GNN model. This graph data includes:
     - Node features (atoms).
     - Edge features (bonds).
     - Labels for training (`self.edge_break_labels`, `self.compiled_probsALL`, etc.).
   - The model uses these data points to learn which bonds are likely to break based on the molecular structure and provided training labels.

3. **Training Process**:
   - Training typically occurs in a supervised learning setup where:
     - The model's predictions (e.g., probabilities of bond breaks) are compared against the ground truth labels (`compiled_probsALL` and related attributes).
     - Loss functions such as cross-entropy or KL divergence might be used to quantify the difference between predicted probabilities and observed labels.
     - The training process iteratively updates the model weights to minimize this loss, thereby learning how to fragment molecules based on observed data.

### Training Integration Example:

- In the **`simulate_and_score` function** of `SimulationFramework.py`, predictions are compared against the compiled probabilities from `Metabolite.py`:
  ```python
  if hasattr(metabolite, base_attr_name):
      groundtruth = getattr(metabolite, base_attr_name).to(self.dev)
  
      stats["cosine_similarity"] = torch.nn.functional.cosine_similarity(prediction["fragment_probs"], groundtruth, dim=0).tolist()
      stats["kl_div"] = torch.nn.functional.kl_div(torch.log(prediction["fragment_probs"]), groundtruth, reduction='sum').tolist()
  ```
  - Here, `prediction["fragment_probs"]` (model output) is compared with `compiled_probsALL` or similar attributes from the `Metabolite` class to assess the model's performance during training or evaluation.

### Where Training Occurs:

- Training typically involves scripts or modules that:
  - Load data, convert it into `Metabolite` objects, and generate fragmentation statistics.
  - Use these data points as input to train the GNN model.
  - Adjust the model weights based on the calculated losses between predicted and actual fragment probabilities.

The **`fiora-predict` script** and related GNN modules (e.g., `GNNCompiler`) handle this integration, where they read the training data, process it using functions like `match_fragments_to_peaks`, and utilize these processed attributes to refine the model’s predictions through iterative training. 

### Summary:

- The attributes generated from `match_fragments_to_peaks` are used as ground truth labels for model training.
- These labels help the GNN model learn which bonds are most likely to break under different conditions, thereby making the fragmentation prediction process more accurate.
- This training pipeline involves both the rule-based fragmentation from `FragmentationTree` and the statistical learning approach from the GNN, providing a comprehensive framework for in silico fragmentation predictions.

